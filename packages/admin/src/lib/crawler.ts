import { spawn } from "child_process"
import { fileURLToPath } from "url"
import { dirname, resolve } from "path"
import {
  crawlerRunsTotal,
  crawlerItemsProcessed,
  crawlerDuration,
  crawlerItemsInProgress,
  crawlerLastRunTime,
} from "./metrics.js"
import { supabase } from "./supabase.js"
import { logStream } from "./log-stream.js"

// ESMì—ì„œ __dirname ëŒ€ì²´
const __filename = fileURLToPath(import.meta.url)
const __dirname = dirname(__filename)

interface CrawlerRunResult {
  success: boolean
  inserted: number
  updated: number
  errors: number
  duration: number
  error?: string
  logs: string[]
}

/**
 * Crawler ì‹¤í–‰ ë° ëª¨ë‹ˆí„°ë§
 */
export async function runCrawler(): Promise<CrawlerRunResult> {
  const startTime = Date.now()
  const logs: string[] = []
  let inserted = 0
  let updated = 0
  let errors = 0
  let success = false

  // í¬ë¡¤ëŸ¬ ì‹¤í–‰ ì‹œì‘ ë¡œê·¸ ì €ì¥
  const runId = await logCrawlerStart()

  logStream.info(`ğŸš€ í¬ë¡¤ëŸ¬ ì‹œì‘ - ${new Date().toLocaleString("ko-KR")}`)
  logs.push(`Starting crawler at ${new Date().toISOString()}`)

  try {
    // í¬ë¡¤ëŸ¬ íŒ¨í‚¤ì§€ ê²½ë¡œ
    const crawlerPath = resolve(__dirname, "../../../crawler")

    // í¬ë¡¤ëŸ¬ë¥¼ spawnìœ¼ë¡œ ì‹¤í–‰í•˜ì—¬ ì‹¤ì‹œê°„ ë¡œê·¸ ìº¡ì²˜
    await new Promise<void>((resolve, reject) => {
      const child = spawn("pnpm", ["sync:date-travel"], {
        cwd: crawlerPath,
        env: process.env,
        shell: true,
        stdio: ["inherit", "pipe", "pipe"],
      })

      let output = ""

      // stdout ì²˜ë¦¬
      child.stdout?.on("data", (data: Buffer) => {
        const text = data.toString()
        output += text
        logs.push(...text.split("\n").filter(line => line.trim()))

        // ì‹¤ì‹œê°„ ë¡œê·¸ ìŠ¤íŠ¸ë¦¬ë°
        text.split("\n").forEach(line => {
          const trimmed = line.trim()
          if (trimmed) {
            if (trimmed.includes("âœ…") || trimmed.includes("âœ¨")) {
              logStream.success(trimmed)
            } else if (trimmed.includes("âŒ") || trimmed.includes("ì˜¤ë¥˜")) {
              logStream.error(trimmed)
            } else if (trimmed.includes("âš ï¸") || trimmed.includes("ê²½ê³ ")) {
              logStream.warning(trimmed)
            } else if (trimmed.includes("ğŸ“Š") || trimmed.includes("%")) {
              logStream.progress(trimmed)
            } else {
              logStream.info(trimmed)
            }
          }
        })
      })

      // stderr ì²˜ë¦¬
      child.stderr?.on("data", (data: Buffer) => {
        const text = data.toString()
        output += text
        logs.push(...text.split("\n").filter(line => line.trim()))
        logStream.error(text.trim())
      })

      child.on("close", code => {
        // ê²°ê³¼ íŒŒì‹±
        const insertedMatch = output.match(/ì´ ì‚½ì…:\s*([\d,]+)/i) || output.match(/ì‚½ì…\s+(\d+)/i)
        const updatedMatch =
          output.match(/ì´ ì—…ë°ì´íŠ¸:\s*([\d,]+)/i) || output.match(/ì—…ë°ì´íŠ¸\s+(\d+)/i)
        const errorsMatch = output.match(/ì´ ì˜¤ë¥˜:\s*(\d+)/i) || output.match(/ì˜¤ë¥˜\s+(\d+)/i)

        if (insertedMatch) inserted = parseInt(insertedMatch[1].replace(/,/g, ""), 10)
        if (updatedMatch) updated = parseInt(updatedMatch[1].replace(/,/g, ""), 10)
        if (errorsMatch) errors = parseInt(errorsMatch[1], 10)

        if (code === 0) {
          success = errors === 0 || inserted > 0 || updated > 0
          resolve()
        } else {
          reject(new Error(`í¬ë¡¤ëŸ¬ê°€ ì¢…ë£Œ ì½”ë“œ ${code}ë¡œ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤`))
        }
      })

      child.on("error", error => {
        reject(error)
      })
    })

    // ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
    crawlerRunsTotal.inc({ status: success ? "success" : "error" })
    crawlerItemsProcessed.inc({ action: "inserted" }, inserted)
    crawlerItemsProcessed.inc({ action: "updated" }, updated)
    crawlerItemsProcessed.inc({ action: "error" }, errors)

    const duration = (Date.now() - startTime) / 1000
    crawlerDuration.observe(duration)
    crawlerLastRunTime.set(Date.now() / 1000)

    // í¬ë¡¤ëŸ¬ ì‹¤í–‰ ì™„ë£Œ ë¡œê·¸ ì €ì¥
    await logCrawlerEnd(runId, {
      success,
      inserted,
      updated,
      errors,
      duration,
      logs: logs.slice(-100), // ìµœê·¼ 100ì¤„ë§Œ ì €ì¥
    })

    return {
      success,
      inserted,
      updated,
      errors,
      duration,
      logs: logs.slice(-50), // ìµœê·¼ 50ì¤„ë§Œ ë°˜í™˜
    }
  } catch (error: any) {
    const duration = (Date.now() - startTime) / 1000
    const errorMessage = error.message || String(error)

    logs.push(`Error: ${errorMessage}`)
    logs.push(error.stack || "")

    crawlerRunsTotal.inc({ status: "error" })
    crawlerDuration.observe(duration)
    crawlerLastRunTime.set(Date.now() / 1000)

    // í¬ë¡¤ëŸ¬ ì‹¤í–‰ ì‹¤íŒ¨ ë¡œê·¸ ì €ì¥
    await logCrawlerEnd(runId, {
      success: false,
      inserted,
      updated,
      errors: errors + 1,
      duration,
      error: errorMessage,
      logs: logs.slice(-100),
    })

    return {
      success: false,
      inserted,
      updated,
      errors: errors + 1,
      duration,
      error: errorMessage,
      logs: logs.slice(-50),
    }
  }
}

/**
 * í¬ë¡¤ëŸ¬ ì‹¤í–‰ ì‹œì‘ ë¡œê·¸ ì €ì¥
 */
async function logCrawlerStart(): Promise<string> {
  try {
    const { data, error } = await supabase
      .from("crawler_runs")
      .insert({
        started_at: new Date().toISOString(),
        status: "running",
      })
      .select("id")
      .single()

    if (error) {
      console.error("Failed to log crawler start:", error)
      return ""
    }

    return data.id
  } catch (error) {
    console.error("Failed to log crawler start:", error)
    return ""
  }
}

/**
 * í¬ë¡¤ëŸ¬ ì‹¤í–‰ ì™„ë£Œ ë¡œê·¸ ì €ì¥
 */
async function logCrawlerEnd(
  runId: string,
  result: {
    success: boolean
    inserted: number
    updated: number
    errors: number
    duration: number
    error?: string
    logs: string[]
  }
): Promise<void> {
  if (!runId) return

  try {
    const { error } = await supabase
      .from("crawler_runs")
      .update({
        completed_at: new Date().toISOString(),
        status: result.success ? "completed" : "failed",
        items_inserted: result.inserted,
        items_updated: result.updated,
        items_errors: result.errors,
        duration_seconds: result.duration,
        error_message: result.error || null,
        logs: result.logs,
      })
      .eq("id", runId)

    if (error) {
      // ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ëŠ” ì¡°ìš©íˆ ì²˜ë¦¬
      if (error.message?.includes("fetch failed") || error.message?.includes("ENOTFOUND")) {
        return // ì¡°ìš©íˆ ì‹¤íŒ¨
      }
      console.error("Failed to log crawler end:", error)
    }
  } catch (error: any) {
    // ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ëŠ” ì¡°ìš©íˆ ì²˜ë¦¬
    if (error?.message?.includes("fetch failed") || error?.message?.includes("ENOTFOUND")) {
      return // ì¡°ìš©íˆ ì‹¤íŒ¨
    }
    console.error("Failed to log crawler end:", error)
  }
}

/**
 * ìµœê·¼ í¬ë¡¤ëŸ¬ ì‹¤í–‰ ê¸°ë¡ ì¡°íšŒ
 */
export async function getCrawlerRuns(limit: number = 10) {
  try {
    const { data, error } = await supabase
      .from("crawler_runs")
      .select("*")
      .order("started_at", { ascending: false })
      .limit(limit)

    if (error) {
      // ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ëŠ” ì¡°ìš©íˆ ì²˜ë¦¬ (Supabase ì—°ê²° ë¶ˆê°€ ì‹œ)
      if (error.message?.includes("fetch failed") || error.message?.includes("ENOTFOUND")) {
        logStream.warning("Supabase ì—°ê²° ë¶ˆê°€ - ì‹¤í–‰ ê¸°ë¡ì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        return []
      }
      console.error("Failed to get crawler runs:", error)
      return []
    }

    return data || []
  } catch (error: any) {
    // ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ëŠ” ì¡°ìš©íˆ ì²˜ë¦¬
    if (error?.message?.includes("fetch failed") || error?.message?.includes("ENOTFOUND")) {
      logStream.warning("Supabase ì—°ê²° ë¶ˆê°€ - ì‹¤í–‰ ê¸°ë¡ì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
      return []
    }
    console.error("Failed to get crawler runs:", error)
    return []
  }
}
